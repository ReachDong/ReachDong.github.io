<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[gn编译hello_world]]></title>
    <url>%2F2020%2F09%2F17%2Fgn%E7%BC%96%E8%AF%91hello-world%2F</url>
    <content type="text"><![CDATA[编译第一个hello,world 使用GN 编译第一个hello，world创建一个工程目录temp，在工程目录里面创建hellow_world.cpp 文件并完成编写。切换到temp 目录下。 在工程目录temp下,将下载的gn/文件夹下examples中的配置文件拷贝到当前目录，这个配置文件中包含BUILDCONFIG.gn等文件，属于对当前工程编译环境的设置，是gn编译的基础。 1cp -rf gn/examples/simple_build/build ./ 在工程目录中创建BUILD.gn文件，用来指定待编译的文件 1echo "executable(\"hello\") &#123; sources = [\"hello_world.cpp\"] &#125;" &gt; BUILD.gn 通过gn 命令 生成ninja 12345gn gen out/default# ninja 编译生成exeninja -C out/default -v # 运行exe./out/default/hello_world]]></content>
      <categories>
        <category>GN工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[安装GN环境流程与命令]]></title>
    <url>%2F2020%2F09%2F12%2FGNGN-compile%2F</url>
    <content type="text"><![CDATA[GN工具安装的命令介绍 GN工具的安装命令如下12345678910111213141516171819202122232425262728293031323334353637383940# 安装C++环境sudo apt-get install gcc# g++安装失败，提示依赖就换源，https://mirrors.tuna.tsinghua.edu.cn/sudo apt-get install g++# 查看安装结果gcc --versiong++ --version################################################################### 安装依赖 re2capt-get install re2cre2c --version #查看安装的版本################################################################## 下载ninjiagit clone https://github.com/ninja-build/ninja.git# 执行编译脚本./configure.py --bootstrap# 放如系统文件夹下sudo cp ./ninja /usr/bin # 查看是否成功ninja --version################################################################## 安装clangsudo apt-get install clang################################################################## 安装gn 可选：https://github.com/timniederhausen/gngit clone https://chromium.googlesource.com/chromium/src/tools/gn# 执行编译脚本./build/gen.py#ninjia 构建 编译结束后，gn程序就在gn/out目录中ninja -C out# 结果复制到系统目录sudo cp ./out/gn /usr/bin# 查看是否成功gn --version1834 (09fffae49)]]></content>
      <categories>
        <category>GN工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[xgboost]]></title>
    <url>%2F2019%2F09%2F03%2Fxgboost%2F</url>
    <content type="text"><![CDATA[xgboost 节点值的选择与分裂 xgboost 节点值的选择以及分裂方法的选择在上篇文章里引用的图片上，没有对打分函数进行具体解释，本节具体说一下，今天面试问道为什么xgboost是二阶泰勒展开，我觉得是和这一部分有关。 损失函数xgboost 的损失函数 如何确定一个节点的值 目标就是找到 f 使得上式达到最小，通过泰勒展开以后，得到下式。其中需要注意一点 l作为loss 其自变量是$\hat y ^{(t-1)}$ 是泰勒展开中的$x_0$ 而f可以看作是增加量，即泰勒展开中的$\Delta x$ 所以得到下式， $ y ^{(t-1)}$ 是常数省略与f 无关的项，得到下式： 又因为f 其实就是w ,所以整理得到按照节点划分的目标函数，最终这个二次函数直接求最值点，就是这个节点的w值。 如何分裂节点通过贪心算法，生成树时，计算左子树，右子树的误差和，和未分裂的进行比较，确定分裂是否合理，然后将每一种分裂，每一次可能进行遍历，进而选择最佳分裂 参考]]></content>
      <categories>
        <category>true</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[反向传导]]></title>
    <url>%2F2019%2F08%2F19%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[RNN 与 反向传播 反向传播的思想梯度下降对于一个固定的函数，在其他变量不变的情况下，观测点沿着函数任一个自变量的负梯度方向移动就会使得观测点的函数值下降，同理同时沿着每个自变量的负梯度构成的负梯度方向移动，函数值就会很快的下降。 对于机器学习，一般的决策函数可以表示为$\hat y = f(x,\theta )$ 其中x是已知输入，y是输出，$\theta$ 是参数，但是在给定训练集和label时，其实自变量就是这个参数，我们的目标是找到一个合适的参数，让模型成为到我们想要的样子，所以我们把‘想要的样子’定义为与真实label 的差距，也就是目标函数，$L(y_i,\hat y_i)$ ,我们用它来衡量，模型与我们期望的差距，差距越小越好。所以目标函数可以看作是L为因变量，$\theta$ 是自变量，我们的终极目标是求L 的最小值时的 $\theta$ 的值。在预测是参数是固定的，此时自变量又变回了x.所以参数的跟新与L的数值结果没有关系，只与它的导数以及当前参数值有关。 反向传播其实传播只是一种通俗，形象的描述，其本质是链式法则，计算目标L对每个参数的导数，然后更新参数。 RNN 举例介绍反向传播 z^t = U* X^t+ W* h^{t-1 } + bh^t = \delta(z^t)o^t = V * h^t + c\hat {y^t} = \sigma(o^t)L =\sum_t L_t= \sum_t loss(y_t, \hat y_t )(由于参数W等是公用的，通过L的表达式可以得到每更新一次参数，就是把$L_t$ 的所有分量的梯度加起来)反向对于参数（目前的自变量）W,V,U,b,c,，分别计算梯度，从举例L最近的分别求导，首先L 对$\hat y_i$ 求导，然后对c求导，按照一般的链式求导法则，很容易计算得到 \sum_t \frac{dL}{dy_t} * \frac {dy_t}{do^t} * \frac {do^t}{dc}当L是交叉熵，$\sigma$ 是softmax的时候,结果为：$\sum_t (\hat y_t- y_t)$同理容易求得对V的导数。 比较难求的是W,U,b 这里以W为例，进行求解。显然W的因变量是$h_t$ ,而$h_t$ 的因变量，有两个，一个是下一层的$L_{t+1}$ (因为影响了下一层的输入$h_{t+1}$) 另一个是这一层的$L_t$ 的. 所以L对于$h_t$的求导会有两个部分（$h_t$对参数W，U，b 求导很简单，直接求导）首先计算L对$h_t$的导数：对于最后一层,T时刻，后续没有$h_{T+1}$所以： \frac{dL_T}{dy_T} * \frac{dy_T}{do^T} * \frac{do^T}{dh_T} = (\hat y_T- y_T)* V当t小于T时，令f(t)=\frac{dL}{dh_t} f(t) = \frac{dL_t}{dy_t} * \frac{dy_t}{do^t} * \frac{do^t}{dh_t} +\frac{dL_{t+1}}{dy_{t+1}} * \frac{dy_{t+1}}{do^{t+1}} * \frac{do^{t+1}}{dh_{t+1}}* \frac{dh^{t+1}}{dh_t}f(t) = (\hat y_t- y_t)* V+f(t+1)* \delta'(h^{t+1})* W显然、$\frac{dh^t}{dW} =h_{t-1}$所以对于，每个L分量$L_t$ 对W求导为： f(t)* \frac{dh^t}{z^t}* \frac{z^t}{dW}=f(t)* \frac{dh^t}{z^t}* W所以 \frac{dL}{dW} = \sum_t f(t)* \frac{dh^t}{z^t}* WW_{new} =W_{old} - \frac{dL}{dW_{old}}同理容易求得其他参数。所以可以看出是每一个序列，更新一次RNN的参数。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[GBDT与XGboost]]></title>
    <url>%2F2019%2F08%2F17%2Fboosting%2F</url>
    <content type="text"><![CDATA[对这两个算法进行简单的记录 GBDT思路： 使用多个cart树，对数据进行拟合，每个树都用来拟合上一步的负梯度（后面解释），最后将每一个cart树的结果累加（加权），得到预测结果。 需要理解的几个问题： 为什么拟合负梯度，和拟合误差一样吗？（回归问题） 如何拟合负梯度 如何确定每个叶子节点的值 加权求和是什么意思 正则化的问题 拟合负梯度（解决前两个问题）首先对于一个函数来说，沿着负梯度方向会让函数值降低，对于一般问题，损失函数L是关于参数$\theta$ 的函数，一般做法是对参数求导，然后更新参数，得到更好的决策函数，进而降低误差。对于GBDT来说我们提升的不是参数$\theta$ 而是决策函数本身（换句话说参数就是树本身），具体来说。针对损失函数的：一般方法的损失更新： L(y_i, f(x_i,\theta^{[i]})=L(y_i, f(x_i,\theta^{[i-1]} - \left. \frac{dL}{d\theta} \right| _{\theta={\theta^{[i-1]}}})对于GBDT的损失更新： L(y_i, f_t(x_i)=L(y_i, f_{t-1}(x_i) - \left. \frac{dL}{df} \right|_{f={f_{t-1}}})所以构建GBDT树的过程：初始：假定初始的$f_0$ 是把所有的输入都预测为0，那么通过损失函数对f的导数的解析式可以得到负梯度的方程$- \left. \frac{dL}{df} \right|_{f={f_0}}$ ,_ 输入$f_{0}$ 就是初始cart树预测的结果（也就是0）,构建第二棵树： 在上一步的基础上计算得到了负梯度，将负梯度作为新的label ,按照cart 树的构造方法，得到第二棵树，并为每个叶子节点确定输出的值（后面介绍）。然后更新下一个负梯度$- \left. \frac{dL}{df} \right|_{f={f_1}}$ ,_ 其中$f_1$ 是这第二棵树的预测结果。用新的到的负梯度作为新的label ,如此迭代下去……通过导数的计算已经包含了L的信息 也就是原始label 的信息。在来说一下误差和负梯度的问题（这里的误差是只真实y与预测的$\hat y$ 的差的绝对值 ），当L损失函数为均方误差时，L对f 的导数就是这个误差，所以用mse 作为损失函数的时候，每次更新的负梯度就是求这棵树对$x_i$ 的预测与这棵树的输入 label 的差的绝对值。 针对分类问题也是一样的，二分类问题可以看作是对于每一个类别概率的回归，每个叶子节点的值代表一个连续值，通过类似sigmoid转化成概率，此时对于目标函数一般使用对数似然损失函数$y\epsilon \{-1,1\}$ L(y,f(x))=log(1+exp(-yf(x))) 多分类 同理 类似softmax 用一阶段泰勒展开式，更新参数或者方程，就是沿着负梯度更新，二阶泰勒展开呢 就是用负梯度除以二阶导数的方向更新，GBDT只用了一阶，Xgboost用的二阶 如何确定每个叶子节点的值 找到一个值c，当这个叶子节点中的所有样本都被预测为c时，对于该叶子节点中的每个样本x，计算其误差，使得误差的累加和最小，那么c为这个叶子节点的值。 如何正则（权重）加入正则项：对于个树，给一个权重v（0到1之间）, 预测结果要乘以v ，才是该树的最终预测结果。 抽样，确定超参数（树的深度等参数） cart树的剪枝等 GBDT与随机森林 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。 组成随机森林的树可以并行生成；而GBDT只能是串行生成。 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。 随机森林对异常值不敏感；GBDT对异常值非常敏感。原因是当前的错误会延续给下一棵树。 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。 随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。 RF不需要进行数据预处理，即特征归一化。而GBDT则需要进行特征归一化。 为什么RF的树深度比GBDT深很多对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。 对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树 xgboost思路与GBDT类似，可以说是在基础上更新的新方法，加入了正则，二阶泰勒展开信息等，具体差别后面总结。 目标函数与正则项loss 函数由二阶泰勒展开近似代替，通过求目标函数（二次函数）的极值点的到$L $ 的最小值，以此作为打分函数的依据（分裂节点的依据）。 在GBDT原有的loss 的基础上，增加了正则项： 算法流程与GBDT一样，不同点是算法的目标函数，以及分裂每个节点的方法。和传统的boosting tree模型一样，xgboost的提升模型也是采用的残差（或梯度负方向），不同的是分裂结点选取的时候不一定是最小平方损失。 对于稀疏值，缺失值通过分别计算假设其属于$G_L$ 或者属于$G_R$ 选择最合适的。 GBDT 与 xgboostxgboost 支持自定义目标函数 只要二阶可导就行。 怎么防过拟合目标函数中的正则项，行抽样，列抽样，shrinkage（在一个树结束的结果上乘以一个0-1之间的数） xgboost与深度学习对比同的机器学习模型适用于不同类型的任务。深度神经网络通过对时空位置建模，能够很好地捕获图像、语音、文本等高维数据。而基于树模型的XGBoost则能很好地处理表格数据，同时还拥有一些深度神经网络所没有的特性（如：模型的可解释性、输入数据的不变性、更易于调参等）。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hexo_公式_图片]]></title>
    <url>%2F2019%2F08%2F07%2Fhexo-%E5%85%AC%E5%BC%8F-%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[向博客中添加图片和公式 向博客中添加图片更改站点配置文件将站点的_config.yml文件中的配置项post_asset_folder设为true 1post_asset_folder: true 存放照片执行命令$ hexo new post_name，在source/_posts中会生成文章post_name.md和同名文件夹post_name。将图片资源放在post_name中，文章就可以使用相对路径引用图片资源了。 在markdown 中插入照片图片在文章和首页中同时显示，可以使用标签插件语法。_posts/post_name/image.jpg这张照片可以用以下方式访问：1&#123;% asset_img image_name.jpg (This is an image) %&#125; 其中： asset_img 是关键字 后面接空格然后加图片在同名文件夹中的名字，后面括号内的内容可省略，是图片在网页中显示的名字 添加公式更换 Hexo 的 markdown 渲染引擎，hexo-renderer-kramed 引擎是在默认的渲染引擎 hexo-renderer-marked 的基础上修改了一些 bug ，两者比较接近，也比较轻量级12$ npm uninstall hexo-renderer-marked --save$ npm install hexo-renderer-kramed --save 更改站点配置文件然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为 hexo-renderer-kramed 引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的 escape 变量的值做相应的修改： 12 //escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,escape: /^\\([`*\[\]()#$+\-.!_&gt;])/, 这一步是在原基础上取消了对\,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改 12 //em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 重新启动hexo（先clean再generate） 更改主题配置文件在 Next 主题中开启 MathJax 开关, 找到mathjax12345mathjax: enable: true per_page: true engine: mathjax cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML 在文章的头里 添加mathjax： true在根目录下scaffolds/post.md 添加1mathjax: true 之所以要在文章头里设置开关，是因为考虑只有在用到公式的页面才加载 Mathjax，这样不需要渲染数学公式的页面的访问速度就不会受到影响了。没有公式的文章用false 即可。之前生成的页面需要加载mathjax的需要手动添加上述文章头，确保开关为true]]></content>
      <categories>
        <category>blog</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[初步学习EM算法]]></title>
    <url>%2F2019%2F08%2F07%2F%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0EM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[EM 算法的部分知识点总结 问题介绍一般的问题：如果概率模型的变量都是观测变量，则给定数据之后，可以直接用极大似然估计法或者贝叶斯估计法来估计模型参数。EM算法解决的问题：存在一个中间过程，观测值是由两步随机过程得到的. 已知三枚硬币 A，B，C ，这些硬币正面出现的概率分别为 。进行如下试验：先投掷硬币 A，若是正面则选硬币 B；若是反面则选硬币 C 。然后投掷被选出来的硬币，投掷的结果如果是正面则记作 1；投掷的结果如果是反面则记作0 。独立重复地 次试验，观测结果为： 1,1,0,1,0,…0,1 。现在只能观测到投掷硬币的结果，无法观测投掷硬币的过程，求估计三硬币正面出现的概率。 其中第一次就是一个隐藏过程。 模型建立 已知最终的观测序列为 假设隐藏层为已知两个过程的模型$P_1,P_2$，但是不知道具体参数,参数集合为$\theta$。 通过极大似然估计的一般过程，目标函数为求公式的最大值： 其中Y 是与Z 有关系的变量，所以公式展开是： 其中$\theta$ 是两个过程中的所有参数。 与一般问题不同的是含有隐藏过程Z ，而它是未观测数据。 模型求解原理目标： 让$L(\theta)$ 迭代的越来越大,直到收敛过程： 带入过程Z 得到新的目标因为log是凸函数，通过jessen不等式得到目标函数的下界：化简得到：最终$L(\theta)$ 的下界为：原问题转化为求上式的最大值时的$\theta$ 值。即：化简（删除了常数项）为：最终原问题转化为上述问题，其中$\theta$ 是下一次迭代的目标值（未知量），其他均为已知量。 算法第一步E步： 求出上一步的最后的式子的解析式，也就是带入 ,得到关于 $\theta$ 的解析式第二步M步骤： 求出最大的$\theta$ ，更新$\theta^{[i]}$收敛条件是：两次迭代之间差距很小。 求解方法分别对每个参数求偏导，令导数为零，进而求得下一轮的初始值。应用为求解高斯混合模型]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[动态规划]]></title>
    <url>%2F2019%2F08%2F06%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[应用动态规划解决队列问题 问题描述 计算最少出列多少位同学，使得剩下的同学排成合唱队形说明：N位同学站成一排，音乐老师要请其中的(N-K)位同学出列，使得剩下的K位同学排成合唱队形。合唱队形是指这样的一种队形：设K位同学从左到右依次编号为1，2…，K，他们的身高分别为T1，T2，…，TK， 则他们的身高满足存在i（1&lt;=i&lt;=K）使得T1]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[build_blog]]></title>
    <url>%2F2019%2F08%2F05%2Fbuild-blog%2F</url>
    <content type="text"><![CDATA[基于hexo github 搭建主题为nextT的博客 github 搭建blog创建github 库创建于用户名相同的github库 命名为user_name.github.io 安装Node.js方法百度 安装Hexo更换nmp 为淘宝源123$ npm config set registry https://registry.npm.taobao.org/$ npm config get registry 安装 hexo1npm install hexo -g 创建hexo文件夹定位到自己创建的blog 文件夹1/blog $ hexo init 安装一些组件1$ nmp install 将Hexo 与github连接设置好git, 可百度方法将blog 文件夹下面的站点配置文件_config.yml 进行修改1234eploy: type: git repository: git@github.com:user_name/user_name.github.io.git branch: master 拷贝主题NexT通过git clone 命令进行， 主题会被放在theme 文件夹下12$ cd your-hexo-site$ git clone https://github.com/iissnan/hexo-theme-next themes/next 启动主题与所有 Hexo 主题启用的模式一样。 当 克隆/下载 完成后，打开 站点配置文件， 找到 theme 字段，并将其值更改为 next 1theme: next 配置主题见链接 NexT 其他文件内容与文件目录介绍hexo 目录中_config.yml 为站点配置文件，source/_post/ 文件夹是存放.md静态文件的目录，其中的.md文件的开头如下其中第三行左边的是类别的关键字（类别的应为就是categoties），右边是具体的类别,不同文件中冒号，左边的是固定的，右边根据需要写所属类别。改模板在\blog\scaffolds\post.md中123title: build_blogdate: 2019-08-05 21:43:01categories: &quot;blog&quot; hexo 常用命令hexo 的所有操作都在git bash 上进行 并且定位在/blog 目录下1234hexo g //生成静态网页并存储在public文件夹下hexo clean // 删除上述文件夹下的文件hexo s -p 5000 //指定本地端口5000 本地查看网页hexo d 上传网页 将public/2019...文件夹下的静态网页上传github]]></content>
      <categories>
        <category>blog</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++容器]]></title>
    <url>%2F2019%2F08%2F05%2FC-%E5%AE%B9%E5%99%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[关于初学C++容器的的一些方法的简单记录 容器的初始化与索引数据类型：vector初始化1234vector&lt;int&gt; my_vec&#123;1,2,3&#125;;vector&lt;int&gt; my_vec(10,5);// 初始化10个5vector&lt;int&gt; my_vec(10); // 初始化10个位置vector&lt;int&gt;my_sec&#123;my_vec.begin()+2,my_vec.end()-1&#125;; //通过my_vec进行初始化[2:-1) map初始化 12map&lt;string, int&gt; my_map&#123;&#123; , &#125; , &#123; , &#125;&#125;; //map 存储pair 类型作为元素// map 与python类似 my_map【】 索引可添加元素 数据类型 对于容器的索引，例如vec[index]. index 是unsign类型，类型名字是size_t，当与int类型进行比较，以及运算是容易出错。 12345 vector&lt;int&gt; ans(10,5);vector&lt;int&gt; my_v&#123;ans.begin()+2,ans.end()-5&#125;;//会报错no matching function for call to 'min' auto anss = min(5,ans.size()); ans.size()不是int类型 转化为int 才能比较。auto anss = min(5,ans.size()); 容器的增删改查加排序添加元素 添加数据可以通过容器自带的类似于push_back(), insert()方法；或者inserter() 这样的泛型函数； 1234vector&lt;int&gt; vec&#123;1,2,3,4&#125;;auto it = vec.insert(vec.begin()+1, 10); // 返回的it 是插入位置(10 所在的位置)的迭代器， 在第一个参数（迭代器）前面插入后面参数的内容it = vec.insert(vec.end(), 3, 4); //插入通过构造函数调用后两个参数的返回值（3个4）插入到vec 末尾it = vec.insert(vec.end(),vec2.begin(), vec2.end()); // 将vec2插入到vec1里, 类似python 里面的extend 删除元素123vector&lt;int&gt;::iterator it_1;auto it = vec.erase(it_1); //删除迭代器所指的元素，返回删除元素位置后一位的迭代器。 修改可以通过迭代器的解引用*it 进行索引和修改 也可以是下标索引，map索引的时候可能会添加心的pair 查找123456789101112131415// 对于string vec 调用自己的find 方法auto it = vec.find(target_val, start_index, end_index); // 从索引范围内查找第一个元素 返回元素所在index 若不在则返回比长度大得多的整数// find_first_of , find_last_of声明如下size_t find_last_of (const string&amp; str, size_t pos = npos) const noexcept;size_t find_last_of (const char* s, size_t pos = npos) const;size_t find_last_of (const char* s, size_t pos, size_t n) const;size_t find_last_of (char c, size_t pos = npos) const noexcept;// 对于vector 没有find 方法 需要调用 泛型函数 findauto it = find(vec.begin(), vec.end(), target_val); // 输入输出均为迭代器；与string index不同// 对于map 查找可以用find 返回值是key 也可以用count 返回true false; 容器最大值最小值的索引通过返回最大或者最小值的迭代器，然后与begin()迭代器做差得到123vector&lt;int&gt; v(10);auto it = max_element(v.begin(),v.end());int index = it-v.begin(); 排序泛型函数sort(it.begin(), it.end(),my_sort);123456sort(numbers.begin(),numbers.end(),my_sort);static bool my_sort(int A, intB)&#123;return A&lt;B;//A靠前&#125;my_sort函数为成员函数时（成员函数默认第一个参数是*this） 需要加static 作为一般函数（没有this参数）是不需要 容器的泛型函数反转1234vector&lt;int&gt; arrayRever&#123;1,2,3,4&#125;;reverse(arrayRever.begin(),arrayRever.end()); //没有返回值 就地反转vector&lt;int&gt; vec (arrayRever.rbegin(), arrayRever.rend()); // 初始化反转]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C—的小问题]]></title>
    <url>%2F2019%2F08%2F05%2FC-%E9%81%87%E8%A7%81%E7%9A%84%E5%B0%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[介绍了 string 与char int char* 的转化，for 枚举类型，和INT_MAX 类型转换string转const char*12string s ="abc";const char* c_s = s.c_str(); 123string str=“world”;const char *p = str.c_str(); // 要加const或者等号右边用char* const char *p = str.data(); // data():与c_str()类似，但是返回的数组不以空字符终止 const char*转string12const char* c_s ="abc";string s(c_s); string转char*12345string s ="abc";char* c;const int len = s.length();c =new char[len+1];strcpy(c,s.c_str()); char*转string123const char* cpc ="abc";char* pc =new char[100];//足够长strcpy(pc,cpc); 数字转stringauto it = to_string(input) it可以是浮点数，input是char类型是 会得到对应的asicc码； char 转string只能通过初始化string 得到单个字母的string类型1234string str(1,'a')string str; str.push_back('a');stringstream ss;ss&lt;&lt;'a';string str2 = ss.str(); for 枚举用法123456789101112vector &lt;int&gt; vec(10.0);string str="abcdefg";map&lt;string, int&gt; my_map=&#123;&#123;"a", 2&#125;,&#123;"b", 3&#125;&#125;;for(auto &amp;it : vec)&#123;cout&lt;&lt;it&lt;&lt;endl; // 返回的是引用 可以用来修改容器内内容&#125;for(auto it : str)&#123;cout&lt;&lt;it&lt;&lt;endl; // 返回的是char&#125;for(auto it : my_map)&#123;cout&lt;&lt;it.first&lt;&lt;it.second&lt;&lt;endl;//返回的是pair变量&#125; int类型的最大最小值12INT_MAX (INT32_MAX)INT_MIN (INT32_MIN)]]></content>
      <categories>
        <category>C++</category>
      </categories>
  </entry>
</search>
